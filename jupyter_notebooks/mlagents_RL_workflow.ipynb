{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fb5db6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ba541",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1 get input image, in numpy\n",
    "2 convert torch tensor and send to device\n",
    "3 model forward\n",
    "5 get output action\n",
    "6 create unity ActionTuple() and add action to this\n",
    "7 set action to unity, env.set_actions(behavior_name,action_tuple)\n",
    "8 env.step\n",
    "9 update weights after N steps, num_steps_for_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcf20447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T03:21:35.270278Z",
     "start_time": "2023-06-14T03:21:29.281969Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Palaash.HPZ\\anaconda3\\envs\\mlagents\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment created.\n",
      "Name of the environment behavior: stage0?team=0\n",
      "Action space is: Continuous: 0, Discrete: (2, 2, 2, 2)\n",
      "Action size is: 4\n"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "    env.close()\n",
    "except:\n",
    "    pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import ActionTuple, UnityEnvironment as UE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "%matplotlib inline\n",
    "\n",
    "env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "print(\"environment created.\")\n",
    "env.reset()\n",
    "\n",
    "#initialise behavior parameters\n",
    "behavior_name=list(env.behavior_specs)[0]\n",
    "print(f\"Name of the environment behavior: {behavior_name}\")\n",
    "behavior_spec=env.behavior_specs[behavior_name]\n",
    "\n",
    "action_space=env.behavior_specs[behavior_name].action_spec\n",
    "\n",
    "print(f\"Action space is: {action_space}\")\n",
    "# actions space is 2,2,2,2 since each of them can do that or do nothing\n",
    "\n",
    "num_actions=len(action_space[1]) if behavior_spec.action_spec.is_discrete() else len(action_space[0]) \n",
    "print(f\"Action size is: {num_actions}\")\n",
    "\n",
    "# action size is 4 since move forward backward rotate right left are 4 different actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33c19a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T03:22:09.291599Z",
     "start_time": "2023-06-14T03:22:09.267598Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#implement custom model algo\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "#algo and update algo function\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int, int, int],\n",
    "    encoding_size: int,\n",
    "    output_size: int\n",
    "    ):        \n",
    "        \"\"\"\n",
    "        Creates a neural network that takes as input a batch of images (3\n",
    "        dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "        tensors)\n",
    "        \"\"\"\n",
    "        super(CustomModel, self).__init__()\n",
    "        height = input_shape[1]\n",
    "        width = input_shape[2]\n",
    "        initial_channels = input_shape[0]\n",
    "        conv_1_hw = self.get_conv_output_shape((height, width), 8, 4)\n",
    "        conv_2_hw = self.get_conv_output_shape(conv_1_hw, 4, 2)\n",
    "        self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "        self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "        #     self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "        self.dense1 = torch.nn.Linear(6272, encoding_size)\n",
    "        self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_image: torch.tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of model, outputs a torch.tensor of shape (num_agents,num_actions).\n",
    "        \"\"\"\n",
    "        #     print(visual_obs.shape)\n",
    "        conv_1 = torch.relu(self.conv1(input_image))\n",
    "        #     print(conv_1.shape)\n",
    "        conv_2 = torch.relu(self.conv2(conv_1))\n",
    "        #     print(conv_2.shape)\n",
    "        conv_2=conv_2.view(-1,6272)\n",
    "        #     print(conv_2.shape)\n",
    "        #     hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "        hidden = self.dense1(conv_2)\n",
    "        #     print(hidden.shape)\n",
    "        hidden = torch.relu(hidden)\n",
    "        #     print(hidden.shape)\n",
    "        hidden = self.dense2(hidden)\n",
    "        #     print(hidden.shape)\n",
    "        #     print(hidden)\n",
    "        return hidden    \n",
    "    \n",
    "    def get_conv_output_shape(\n",
    "        h_w: Tuple[int, int],\n",
    "        kernel_size: int = 1,\n",
    "        stride: int = 1,\n",
    "        pad: int = 0,\n",
    "        dilation: int = 1,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Using standard formula, computes the height and width of the output of a convolution layer.\n",
    "            \"\"\"\n",
    "            h = floor(\n",
    "              ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "            )\n",
    "            w = floor(\n",
    "              ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "            )\n",
    "            return h, w\n",
    "    \n",
    "    def update_weights():\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc6d6d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-14T03:22:09.968824Z",
     "start_time": "2023-06-14T03:22:09.913097Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CustomModel' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2e407f29afab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_input_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCustomModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# model=VisualQNetwork(input_size,encoding_size,output_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-030bbfc30e66>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_shape, encoding_size, output_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0minitial_channels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mconv_1_hw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conv_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mconv_2_hw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_conv_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_1_hw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_2_hw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mconv_2_hw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-030bbfc30e66>\u001b[0m in \u001b[0;36mget_conv_output_shape\u001b[1;34m(h_w, kernel_size, stride, pad, dilation)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \"\"\"\n\u001b[0;32m     67\u001b[0m             h = floor(\n\u001b[1;32m---> 68\u001b[1;33m               \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdilation\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             )\n\u001b[0;32m     70\u001b[0m             w = floor(\n",
      "\u001b[1;31mTypeError\u001b[0m: 'CustomModel' object does not support indexing"
     ]
    }
   ],
   "source": [
    "#code only works for single agent, if multi agent need to add tracked=-1, refer to old sample code\n",
    "#for multi agent, will need to change the code snippet at the second decision_steps,terminal_steps=env.get_steps(behavior_name)\n",
    "\n",
    "num_episodes=10\n",
    "max_steps=100000 #max steps in 1 episode\n",
    "num_steps_for_update=1000\n",
    "print_how_many_results_in_one_episode=5 #how many lines of intermediate results do you want to see\n",
    "encoding_size=1000\n",
    "\n",
    "decision_steps,terminal_steps=env.get_steps(behavior_name)\n",
    "sample_input_image=decision_steps.obs[0] #in numpy    \n",
    "input_size=sample_input_image.shape\n",
    "output_size=num_actions\n",
    "model=CustomModel(input_size,encoding_size,output_size)\n",
    "# model=VisualQNetwork(input_size,encoding_size,output_size)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "print(f\"model: {model}\")\n",
    "print(f\"device: {device}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    print(\"-\"*30)\n",
    "    print(f\"Start of Episode {episode+1}/{num_episodes}\")\n",
    "    env.reset() \n",
    "    decision_steps,terminal_steps=env.get_steps(behavior_name)\n",
    "    #decision steps tells you which agents are requesting an action (means episode not ended for that agent, for multi agents, some agents end before others)\n",
    "    #terminal steps tells you which agents have reached terminal step (means episode ended)\n",
    "    episode_reward=0 \n",
    "    \n",
    "    for i in range(max_steps):\n",
    "#         if len(decision_steps)!=0: #means theres still agents requesting decision\n",
    "        #get the input RGB images of the agent\n",
    "        input_images=decision_steps.obs[0] #in numpy\n",
    "        input_images=torch.from_numpy(input_images) #convert to torch #env is in cpu, so the input images are in cpu\n",
    "        input_images=input_images.detach() #to prevent calc of gradients to input images durign backprop\n",
    "        input_images=input_images.to(device) #send to gpu since model is in gpu\n",
    "        #generate an action for the agent, should be in shape (num_agents,num_actions)\n",
    "        output_actions=model.forward(input_images) #currently in gpu and type torch.tensor\n",
    "        #send back to cpu and convert to numpy\n",
    "#         output_actions=output_actions.to(\"cpu\") \n",
    "        outputs_actions_detached=output_actions.detach().numpy()\n",
    "        #setup mlagents ActionTuple and add actions and set actions\n",
    "        action_tuple=ActionTuple()\n",
    "        action_tuple.add_discrete(outputs_actions_detached)\n",
    "        env.set_actions(behavior_name,action_tuple)\n",
    "        env.step() #moves the simulation by 1 step\n",
    "\n",
    "        if i%num_steps_for_update==0:\n",
    "            #update the weights of the model\n",
    "            model.update_weights()\n",
    "\n",
    "        #run this again since this might be the step that is the last and agent moves to terminal step\n",
    "        decision_steps,terminal_steps=env.get_steps(behavior_name)\n",
    "        if len(decision_steps)!=0:\n",
    "            episode_rewards+=decision_steps[0].reward\n",
    "        else: #means agent ended and now in terminal_steps\n",
    "            episode_rewards+=terminal_steps[0].reward\n",
    "            print(f\"Terminal step reached. End episode: Total num of steps: {i+1}, Total reward for episode {episode+1}/{num_episodes} is {episode_rewards}\")                \n",
    "            break #break out of current episode loop regardless of max steps hit or not\n",
    "\n",
    "        print_divisor=1/(print_how_many_results_in_one_episode+1)\n",
    "        if i%int(max_steps*print_divisor)==0:\n",
    "            if i==0 or i==max_steps-1:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"At step {i+1}, Cumulative reward thus far {episode+1}/{num_episodes} is {episode_rewards}\")                \n",
    "\n",
    "        if i==max_steps:\n",
    "            print(f\"Max steps reached, End episode prematurely. Total reward for episode {episode+1}/{num_episodes} is {episode_rewards}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86083c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# example q learning algo from online source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b188e9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## pytorch model - q learning algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b34982de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:20:27.738059Z",
     "start_time": "2023-06-13T01:20:27.713868Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "from math import floor\n",
    "from torch.nn import Parameter\n",
    "\n",
    "\n",
    "class VisualQNetwork(torch.nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    input_shape: Tuple[int, int, int],\n",
    "    encoding_size: int,\n",
    "    output_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Creates a neural network that takes as input a batch of images (3\n",
    "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
    "    tensors)\n",
    "    \"\"\"\n",
    "    super(VisualQNetwork, self).__init__()\n",
    "    height = input_shape[1]\n",
    "    width = input_shape[2]\n",
    "    initial_channels = input_shape[0]\n",
    "    conv_1_hw = self.conv_output_shape((height, width), 8, 4)\n",
    "    conv_2_hw = self.conv_output_shape(conv_1_hw, 4, 2)\n",
    "    self.final_flat = conv_2_hw[0] * conv_2_hw[1] * 32\n",
    "    self.conv1 = torch.nn.Conv2d(initial_channels, 16, [8, 8], [4, 4])\n",
    "    self.conv2 = torch.nn.Conv2d(16, 32, [4, 4], [2, 2])\n",
    "    self.dense1 = torch.nn.Linear(self.final_flat, encoding_size)\n",
    "    self.dense2 = torch.nn.Linear(encoding_size, output_size)\n",
    "\n",
    "  def forward(self, visual_obs: torch.tensor):\n",
    "    print(visual_obs.shape)\n",
    "    conv_1 = torch.relu(self.conv1(visual_obs))\n",
    "    print(conv_1.shape)\n",
    "    conv_2 = torch.relu(self.conv2(conv_1))\n",
    "    print(conv_2.shape)\n",
    "    hidden = self.dense1(conv_2.reshape([-1, self.final_flat]))\n",
    "    print(hidden.shape)\n",
    "    hidden = torch.relu(hidden)\n",
    "    print(hidden.shape)\n",
    "    hidden = self.dense2(hidden)\n",
    "    print(hidden.shape)\n",
    "    return hidden\n",
    "\n",
    "  @staticmethod\n",
    "  def conv_output_shape(\n",
    "    h_w: Tuple[int, int],\n",
    "    kernel_size: int = 1,\n",
    "    stride: int = 1,\n",
    "    pad: int = 0,\n",
    "    dilation: int = 1,\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Computes the height and width of the output of a convolution layer.\n",
    "    \"\"\"\n",
    "    h = floor(\n",
    "      ((h_w[0] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    w = floor(\n",
    "      ((h_w[1] + (2 * pad) - (dilation * (kernel_size - 1)) - 1) / stride) + 1\n",
    "    )\n",
    "    return h, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a09b8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## create classes to store env data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "397897e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:20:29.093649Z",
     "start_time": "2023-06-13T01:20:29.074284Z"
    },
    "code_folding": [
     4
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, List\n",
    "\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "  \"\"\"\n",
    "  An experience contains the data of one Agent transition.\n",
    "  - Observation\n",
    "  - Action\n",
    "  - Reward\n",
    "  - Done flag\n",
    "  - Next Observation\n",
    "  \"\"\"\n",
    "\n",
    "  obs: np.ndarray\n",
    "  action: np.ndarray\n",
    "  reward: float\n",
    "  done: bool\n",
    "  next_obs: np.ndarray\n",
    "\n",
    "# A Trajectory is an ordered sequence of Experiences\n",
    "Trajectory = List[Experience]\n",
    "\n",
    "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
    "Buffer = List[Experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361c735",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## trainer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6a33fa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we can create our trainer class. The role of this trainer is to collect data from the Environment according to a Policy, and then train the Q-Network with that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18d27939",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:20:32.366641Z",
     "start_time": "2023-06-13T01:20:32.340704Z"
    },
    "code_folding": [
     5,
     54,
     77
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "  @staticmethod\n",
    "  def generate_trajectories(\n",
    "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Given a Unity Environment and a Q-Network, this method will generate a\n",
    "    buffer of Experiences obtained by running the Environment with the Policy\n",
    "    derived from the Q-Network.\n",
    "    :param BaseEnv: The UnityEnvironment used.\n",
    "    :param q_net: The Q-Network used to collect the data.\n",
    "    :param buffer_size: The minimum size of the buffer this method will return.\n",
    "    :param epsilon: Will add a random normal variable with standard deviation.\n",
    "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
    "    :returns: a Tuple containing the created buffer and the average cumulative\n",
    "    the Agents obtained.\n",
    "    \"\"\"\n",
    "    # Create an empty Buffer\n",
    "    buffer: Buffer = []\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset()\n",
    "    # Read and store the Behavior Name of the Environment\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    # Read and store the Behavior Specs of the Environment\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "\n",
    "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
    "    # trajectories for each Agents\n",
    "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
    "    # Create a Mapping from AgentId to the last observation of the Agent\n",
    "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to the last action of the Agent\n",
    "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
    "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
    "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
    "    # Create a list to store the cumulative rewards obtained so far\n",
    "    cumulative_rewards: List[float] = []\n",
    "\n",
    "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
    "      # Get the Decision Steps and Terminal Steps of the Agents\n",
    "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "      # permute the tensor to go from NHWC to NCHW (batch_size,height,width,num_channels) to (batch_size,num_channels,height,width,)\n",
    "      order = (0, 3, 1, 2)\n",
    "      decision_steps.obs = [np.transpose(obs, order) for obs in decision_steps.obs]\n",
    "      terminal_steps.obs = [np.transpose(obs, order) for obs in terminal_steps.obs]\n",
    "\n",
    "      # For all Agents with a Terminal Step:\n",
    "      for agent_id_terminated in terminal_steps:\n",
    "        # Create its last experience (is last because the Agent terminated)\n",
    "        last_experience = Experience(\n",
    "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
    "          reward=terminal_steps[agent_id_terminated].reward,\n",
    "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
    "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
    "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
    "        )\n",
    "        # Clear its last observation and action (Since the trajectory is over)\n",
    "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
    "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
    "        # Report the cumulative reward\n",
    "        cumulative_reward = (\n",
    "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
    "          + terminal_steps[agent_id_terminated].reward\n",
    "        )\n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "        # Add the Trajectory and the last experience to the buffer\n",
    "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
    "        buffer.append(last_experience)\n",
    "\n",
    "      # For all Agents with a Decision Step:\n",
    "      for agent_id_decisions in decision_steps:\n",
    "        # If the Agent does not have a Trajectory, create an empty one\n",
    "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
    "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
    "\n",
    "        # If the Agent requesting a decision has a \"last observation\"\n",
    "        if agent_id_decisions in dict_last_obs_from_agent:\n",
    "          # Create an Experience from the last observation and the Decision Step\n",
    "          exp = Experience(\n",
    "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
    "            reward=decision_steps[agent_id_decisions].reward,\n",
    "            done=False,\n",
    "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
    "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
    "          )\n",
    "          # Update the Trajectory of the Agent and its cumulative reward\n",
    "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
    "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
    "            decision_steps[agent_id_decisions].reward\n",
    "          )\n",
    "        # Store the observation as the new \"last observation\"\n",
    "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
    "          decision_steps[agent_id_decisions].obs[0]\n",
    "        )\n",
    "\n",
    "      # Generate an action for all the Agents that requested a decision\n",
    "      # Compute the values for each action given the observation\n",
    "      actions_values = (\n",
    "        q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
    "      )\n",
    "      # Add some noise with epsilon to the values\n",
    "      actions_values += epsilon * (\n",
    "        np.random.randn(actions_values.shape[0], actions_values.shape[1])\n",
    "      ).astype(np.float32)\n",
    "      # Pick the best action using argmax\n",
    "      actions = np.argmax(actions_values, axis=1)\n",
    "      actions.resize((len(decision_steps), 1))\n",
    "      # Store the action that was picked, it will be put in the trajectory later\n",
    "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
    "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
    "\n",
    "      # Set the actions in the environment\n",
    "      # Unity Environments expect ActionTuple instances.\n",
    "      action_tuple = ActionTuple()\n",
    "      action_tuple.add_discrete(actions)\n",
    "      env.set_actions(behavior_name, action_tuple)\n",
    "      # Perform a step in the simulation\n",
    "      env.step()\n",
    "    return buffer, np.mean(cumulative_rewards)\n",
    "\n",
    "  @staticmethod\n",
    "  def update_q_net(\n",
    "    q_net: VisualQNetwork,\n",
    "    optimizer: torch.optim,\n",
    "    buffer: Buffer,\n",
    "    action_size: int\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = 1000\n",
    "    NUM_EPOCH = 3\n",
    "    GAMMA = 0.9\n",
    "    batch_size = min(len(buffer), BATCH_SIZE)\n",
    "    random.shuffle(buffer)\n",
    "    # Split the buffer into batches\n",
    "    batches = [\n",
    "      buffer[batch_size * start : batch_size * (start + 1)]\n",
    "      for start in range(int(len(buffer) / batch_size))\n",
    "    ]\n",
    "    for _ in range(NUM_EPOCH):\n",
    "      for batch in batches:\n",
    "        # Create the Tensors that will be fed in the network\n",
    "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
    "        reward = torch.from_numpy(\n",
    "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        done = torch.from_numpy(\n",
    "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
    "        )\n",
    "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
    "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
    "\n",
    "        # Use the Bellman equation to update the Q-Network\n",
    "        target = (\n",
    "          reward\n",
    "          + (1.0 - done)\n",
    "          * GAMMA\n",
    "          * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
    "        )\n",
    "        mask = torch.zeros((len(batch), action_size))\n",
    "        mask.scatter_(1, action, 1)\n",
    "        prediction = torch.sum(q_net(obs) * mask, dim=1, keepdim=True)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        # Perform the backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf4f32fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T01:20:38.267476Z",
     "start_time": "2023-06-13T01:20:33.050537Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment created.\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 16, 31, 31])\n",
      "torch.Size([1, 32, 14, 14])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1728]' is invalid for input of size 6272",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-2ce98fb40b0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_TRAINING_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mnew_exp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_trajectories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_NEW_EXP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-5c190aaaa4f2>\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[1;34m(env, q_net, buffer_size, epsilon)\u001b[0m\n\u001b[0;32m    105\u001b[0m       \u001b[1;31m# Compute the values for each action given the observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m       actions_values = (\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mq_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecision_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m       )\n\u001b[0;32m    109\u001b[0m       \u001b[1;31m# Add some noise with epsilon to the values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-2d45ed46fcf1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, visual_obs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mconv_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_flat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 1728]' is invalid for input of size 6272"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "# env = default_registry[\"GridWorld\"].make()\n",
    "# print(\"GridWorld environment created.\")\n",
    "\n",
    "env =  UE(file_name=\"stage0_160523\\stage0_copy\",seed=1,side_channels=[])\n",
    "print(\"environment created.\")\n",
    "\n",
    "num_actions = 4\n",
    "\n",
    "try:\n",
    "  # Create a new Q-Network.\n",
    "  qnet = VisualQNetwork((3, 64, 84), 126, num_actions)\n",
    "\n",
    "  experiences: Buffer = []\n",
    "  optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "  cumulative_rewards: List[float] = []\n",
    "\n",
    "  # The number of training steps that will be performed\n",
    "  NUM_TRAINING_STEPS = int(os.getenv('QLEARNING_NUM_TRAINING_STEPS', 70))\n",
    "  # The number of experiences to collect per training step\n",
    "  NUM_NEW_EXP = int(os.getenv('QLEARNING_NUM_NEW_EXP', 1000))\n",
    "  # The maximum size of the Buffer\n",
    "  BUFFER_SIZE = int(os.getenv('QLEARNING_BUFFER_SIZE', 10000))\n",
    "\n",
    "  for n in range(NUM_TRAINING_STEPS):\n",
    "    new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "    random.shuffle(experiences)\n",
    "    if len(experiences) > BUFFER_SIZE:\n",
    "      experiences = experiences[:BUFFER_SIZE]\n",
    "    experiences.extend(new_exp)\n",
    "    Trainer.update_q_net(qnet, optim, experiences, num_actions)\n",
    "    _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "    cumulative_rewards.append(rewards)\n",
    "    print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "except KeyboardInterrupt:\n",
    "  print(\"\\nTraining interrupted, continue to next cell to save to save the model.\")\n",
    "finally:\n",
    "  env.close()\n",
    "\n",
    "# Show the training graph\n",
    "try:\n",
    "  plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n",
    "except ValueError:\n",
    "  print(\"\\nPlot failed on interrupted training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef749e5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SARSA algorithm\n",
    "-this is a model free algorithm that does not use any deep learning models, neural networks\n",
    "-tried and tested before but results are very bad since task with RGB inputs require CNN / model based algo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00635e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SARSA (State-Action-Reward-State-Action) is a popular reinforcement learning algorithm that updates Q-values based on the observed state-action pairs and their subsequent state-action pairs. While SARSA is a well-known and widely-used algorithm in the field of reinforcement learning, it is not considered state-of-the-art (SOTA) compared to more advanced algorithms such as Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), or Soft Actor-Critic (SAC).\n",
    "\n",
    "State-of-the-art algorithms in reinforcement learning are often more sophisticated and build upon SARSA-like algorithms by incorporating deep neural networks, value function approximation, policy gradients, or other advanced techniques. These SOTA algorithms have achieved significant breakthroughs and performance improvements in various domains, including game playing, robotics, and control.\n",
    "\n",
    "That being said, SARSA is still relevant and used in certain research and practical applications, especially when dealing with small and discrete state and action spaces. It serves as a good starting point for understanding reinforcement learning concepts and provides a solid foundation for exploring more advanced algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690646ac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SARSA does not use any deep learning neural networks or any ML models.\n",
    "SARSA is a model-free reinforcement learning algorithm that does not rely on neural networks. It is a tabular method that maintains a table (or dictionary) of Q-values, where each entry represents the estimated value of a state-action pair. The Q-values are updated iteratively based on the observed experiences.\n",
    "\n",
    "SARSA is a TD (Temporal Difference) learning algorithm that combines the concepts of on-policy learning and the use of a value function. It interacts with the environment, observes state transitions, selects actions based on a policy (e.g., epsilon-greedy), and updates the Q-values accordingly.\n",
    "\n",
    "While SARSA itself does not utilize neural networks, its basic principles can be extended and combined with neural networks in more advanced algorithms. For example, Deep Q-Networks (DQN) combine SARSA with deep neural networks to handle high-dimensional state spaces and achieve better performance in complex environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "64f9e29e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## START TRAINING ##########\n",
      "Episode 1, Steps taken: 1647, Time elapsed: 164.24765014648438, Total reward: -53.0\n",
      "Episode 2, Steps taken: 366, Time elapsed: 36.56473231315613, Total reward: -4.0\n",
      "Episode 3, Steps taken: 622, Time elapsed: 62.228856563568115, Total reward: -5.0\n",
      "Episode 4, Steps taken: 131, Time elapsed: 13.070374011993408, Total reward: 0.0\n",
      "Episode 5, Steps taken: 171, Time elapsed: 17.062402486801147, Total reward: -2.0\n",
      "Episode 6, Steps taken: 516, Time elapsed: 51.58318328857422, Total reward: -15.0\n",
      "Episode 7, Steps taken: 1101, Time elapsed: 109.98961114883423, Total reward: -5.0\n",
      "Episode 8, Steps taken: 1786, Time elapsed: 178.5037899017334, Total reward: -24.0\n",
      "Episode 9, Steps taken: 4006, Time elapsed: 400.52717185020447, Total reward: -24.0\n",
      "Episode 10, Steps taken: 54, Time elapsed: 5.401919603347778, Total reward: -1.0\n",
      "Episode 11, Steps taken: 627, Time elapsed: 62.6712908744812, Total reward: 0.0\n",
      "Episode 12, Steps taken: 2648, Time elapsed: 264.68278884887695, Total reward: -61.0\n",
      "Episode 13, Steps taken: 508, Time elapsed: 50.71600270271301, Total reward: -2.0\n",
      "Episode 14, Steps taken: 456, Time elapsed: 45.53718709945679, Total reward: 0.0\n",
      "Episode 15, Steps taken: 2162, Time elapsed: 216.18242263793945, Total reward: -60.0\n",
      "Episode 16, Steps taken: 1224, Time elapsed: 122.38585066795349, Total reward: -15.0\n",
      "Episode 17, Steps taken: 894, Time elapsed: 89.36135601997375, Total reward: -20.0\n",
      "Episode 18, Steps taken: 1031, Time elapsed: 103.04617166519165, Total reward: -6.0\n",
      "Episode 19, Steps taken: 1205, Time elapsed: 120.44134879112244, Total reward: -11.0\n",
      "Episode 20, Steps taken: 625, Time elapsed: 62.41037154197693, Total reward: -26.0\n",
      "Episode 21, Steps taken: 733, Time elapsed: 73.25356554985046, Total reward: -20.0\n",
      "Episode 22, Steps taken: 520, Time elapsed: 51.866614818573, Total reward: -10.0\n",
      "Episode 23, Steps taken: 2381, Time elapsed: 238.1050934791565, Total reward: -3.0\n",
      "Episode 24, Steps taken: 282, Time elapsed: 28.142924070358276, Total reward: 0.0\n",
      "Episode 25, Steps taken: 1449, Time elapsed: 144.85826516151428, Total reward: -7.0\n",
      "Episode 26, Steps taken: 1497, Time elapsed: 149.60432243347168, Total reward: -5.0\n",
      "Episode 27, Steps taken: 3533, Time elapsed: 353.2259750366211, Total reward: -75.0\n",
      "Episode 28, Steps taken: 1060, Time elapsed: 105.91630721092224, Total reward: 1.0\n",
      "Episode 29, Steps taken: 849, Time elapsed: 84.81292343139648, Total reward: -2.0\n",
      "Episode 30, Steps taken: 502, Time elapsed: 50.13479042053223, Total reward: -5.0\n",
      "Episode 31, Steps taken: 1882, Time elapsed: 188.09829759597778, Total reward: -36.0\n",
      "Episode 32, Steps taken: 130, Time elapsed: 12.941510438919067, Total reward: -1.0\n",
      "Episode 33, Steps taken: 2051, Time elapsed: 205.0193965435028, Total reward: -81.0\n",
      "Episode 34, Steps taken: 1515, Time elapsed: 151.42072319984436, Total reward: -93.0\n",
      "Episode 35, Steps taken: 2682, Time elapsed: 268.14809799194336, Total reward: -19.0\n",
      "Episode 36, Steps taken: 158, Time elapsed: 15.803518772125244, Total reward: 0.0\n",
      "Episode 37, Steps taken: 865, Time elapsed: 86.42821073532104, Total reward: -13.0\n",
      "Episode 38, Steps taken: 1045, Time elapsed: 104.34958505630493, Total reward: -6.0\n",
      "Episode 39, Steps taken: 592, Time elapsed: 59.14499592781067, Total reward: -16.0\n",
      "Episode 40, Steps taken: 2787, Time elapsed: 278.65657114982605, Total reward: -26.0\n",
      "Episode 41, Steps taken: 744, Time elapsed: 74.31992530822754, Total reward: -14.0\n",
      "Episode 42, Steps taken: 1099, Time elapsed: 109.77983355522156, Total reward: -22.0\n",
      "Episode 43, Steps taken: 649, Time elapsed: 64.79101824760437, Total reward: 0.0\n",
      "Episode 44, Steps taken: 1371, Time elapsed: 137.06313395500183, Total reward: -8.0\n",
      "Episode 45, Steps taken: 2276, Time elapsed: 227.53830647468567, Total reward: -18.0\n",
      "Episode 46, Steps taken: 5004, Time elapsed: 500.3386175632477, Total reward: -24.0\n",
      "Episode 47, Steps taken: 1600, Time elapsed: 159.84335088729858, Total reward: -8.0\n",
      "Episode 48, Steps taken: 237, Time elapsed: 23.618815660476685, Total reward: -3.0\n",
      "Episode 49, Steps taken: 983, Time elapsed: 98.25261616706848, Total reward: -2.0\n",
      "Episode 50, Steps taken: 2606, Time elapsed: 260.5016493797302, Total reward: -43.0\n",
      "Episode 51, Steps taken: 2878, Time elapsed: 287.716096162796, Total reward: -38.0\n",
      "Episode 52, Steps taken: 1816, Time elapsed: 181.51820993423462, Total reward: -4.0\n",
      "Episode 53, Steps taken: 943, Time elapsed: 94.24040150642395, Total reward: -3.0\n",
      "Episode 54, Steps taken: 657, Time elapsed: 65.59089016914368, Total reward: 1.0\n",
      "Episode 55, Steps taken: 1139, Time elapsed: 113.86175465583801, Total reward: -49.0\n",
      "Episode 56, Steps taken: 1462, Time elapsed: 146.1228199005127, Total reward: -10.0\n",
      "Episode 57, Steps taken: 1473, Time elapsed: 147.20626401901245, Total reward: -22.0\n",
      "Episode 58, Steps taken: 596, Time elapsed: 59.58589291572571, Total reward: -13.0\n",
      "Episode 59, Steps taken: 1293, Time elapsed: 129.22635793685913, Total reward: -13.0\n",
      "Episode 60, Steps taken: 1312, Time elapsed: 131.13502097129822, Total reward: -27.0\n",
      "Episode 61, Steps taken: 990, Time elapsed: 98.95857548713684, Total reward: -8.0\n",
      "Episode 62, Steps taken: 432, Time elapsed: 43.06674361228943, Total reward: -7.0\n",
      "Episode 63, Steps taken: 685, Time elapsed: 68.44071340560913, Total reward: -4.0\n",
      "Episode 64, Steps taken: 1823, Time elapsed: 182.21740746498108, Total reward: -32.0\n",
      "Episode 65, Steps taken: 1565, Time elapsed: 156.45092010498047, Total reward: -21.0\n",
      "Episode 66, Steps taken: 357, Time elapsed: 35.59478688240051, Total reward: 0.0\n",
      "Episode 67, Steps taken: 693, Time elapsed: 69.22212147712708, Total reward: 0.0\n",
      "Episode 68, Steps taken: 632, Time elapsed: 63.109787464141846, Total reward: -1.0\n",
      "Episode 69, Steps taken: 554, Time elapsed: 55.350536823272705, Total reward: 0.0\n",
      "Episode 70, Steps taken: 111, Time elapsed: 11.060876607894897, Total reward: 0.0\n",
      "Episode 71, Steps taken: 915, Time elapsed: 91.47173047065735, Total reward: -28.0\n",
      "Episode 72, Steps taken: 1326, Time elapsed: 132.4975242614746, Total reward: -29.0\n",
      "Episode 73, Steps taken: 800, Time elapsed: 79.94695711135864, Total reward: -7.0\n",
      "Episode 74, Steps taken: 305, Time elapsed: 30.454506635665894, Total reward: -16.0\n",
      "Episode 75, Steps taken: 1025, Time elapsed: 102.48965334892273, Total reward: -13.0\n",
      "Episode 76, Steps taken: 610, Time elapsed: 60.92721652984619, Total reward: -8.0\n",
      "Episode 77, Steps taken: 1789, Time elapsed: 178.81897377967834, Total reward: 0.0\n",
      "Episode 78, Steps taken: 870, Time elapsed: 86.8793957233429, Total reward: -24.0\n",
      "Episode 79, Steps taken: 1670, Time elapsed: 166.88979935646057, Total reward: -47.0\n",
      "Episode 80, Steps taken: 3764, Time elapsed: 376.39581537246704, Total reward: -28.0\n",
      "Episode 81, Steps taken: 109, Time elapsed: 10.794786930084229, Total reward: -1.0\n",
      "Episode 82, Steps taken: 1584, Time elapsed: 158.3175778388977, Total reward: -26.0\n",
      "Episode 83, Steps taken: 813, Time elapsed: 81.2489104270935, Total reward: -13.0\n",
      "Episode 84, Steps taken: 311, Time elapsed: 30.986205577850342, Total reward: 0.0\n",
      "Episode 85, Steps taken: 38, Time elapsed: 3.708021640777588, Total reward: -2.0\n",
      "Episode 86, Steps taken: 2685, Time elapsed: 268.3914723396301, Total reward: -14.0\n",
      "Episode 87, Steps taken: 1023, Time elapsed: 104.14883470535278, Total reward: -1.0\n",
      "Episode 88, Steps taken: 1076, Time elapsed: 107.51718854904175, Total reward: -9.0\n",
      "Episode 89, Steps taken: 4517, Time elapsed: 451.6634614467621, Total reward: -45.0\n",
      "Episode 90, Steps taken: 307, Time elapsed: 30.63748264312744, Total reward: 0.0\n",
      "Episode 91, Steps taken: 247, Time elapsed: 24.69119358062744, Total reward: 0.0\n",
      "Episode 92, Steps taken: 1103, Time elapsed: 110.18290591239929, Total reward: -4.0\n",
      "Episode 93, Steps taken: 881, Time elapsed: 88.05892944335938, Total reward: -9.0\n",
      "Episode 94, Steps taken: 2084, Time elapsed: 208.3231074810028, Total reward: -34.0\n",
      "Episode 95, Steps taken: 521, Time elapsed: 51.99503493309021, Total reward: 1.0\n",
      "Episode 96, Steps taken: 478, Time elapsed: 47.737396240234375, Total reward: -2.0\n",
      "Episode 97, Steps taken: 1310, Time elapsed: 130.86641550064087, Total reward: -16.0\n",
      "Episode 98, Steps taken: 556, Time elapsed: 55.58112668991089, Total reward: -8.0\n",
      "Episode 99, Steps taken: 553, Time elapsed: 55.16456961631775, Total reward: -5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/400Episode 100, Steps taken: 2019, Time elapsed: 201.84026408195496, Total reward: -1.0\n",
      "Episode 101, Steps taken: 1913, Time elapsed: 191.21023511886597, Total reward: -35.0\n",
      "Episode 102, Steps taken: 721, Time elapsed: 72.00407552719116, Total reward: -5.0\n",
      "Episode 103, Steps taken: 419, Time elapsed: 41.838796615600586, Total reward: -1.0\n",
      "Episode 104, Steps taken: 842, Time elapsed: 84.11452436447144, Total reward: -10.0\n",
      "Episode 105, Steps taken: 1601, Time elapsed: 160.0652461051941, Total reward: -27.0\n",
      "Episode 106, Steps taken: 1762, Time elapsed: 176.05212664604187, Total reward: -22.0\n",
      "Episode 107, Steps taken: 2971, Time elapsed: 297.0610935688019, Total reward: -41.0\n",
      "Episode 108, Steps taken: 3582, Time elapsed: 358.60643219947815, Total reward: -65.0\n",
      "Episode 109, Steps taken: 1118, Time elapsed: 111.69622564315796, Total reward: -36.0\n",
      "Episode 110, Steps taken: 357, Time elapsed: 35.721726179122925, Total reward: 1.0\n",
      "Episode 111, Steps taken: 56, Time elapsed: 5.517662763595581, Total reward: -6.0\n",
      "Episode 112, Steps taken: 1090, Time elapsed: 109.01114964485168, Total reward: -19.0\n",
      "Episode 113, Steps taken: 616, Time elapsed: 61.63127136230469, Total reward: -10.0\n",
      "Episode 114, Steps taken: 1470, Time elapsed: 147.01653122901917, Total reward: -25.0\n",
      "Episode 115, Steps taken: 299, Time elapsed: 29.869137048721313, Total reward: 1.0\n",
      "Episode 116, Steps taken: 3475, Time elapsed: 347.3919749259949, Total reward: -152.0\n",
      "Episode 117, Steps taken: 2649, Time elapsed: 264.8980212211609, Total reward: -55.0\n",
      "Episode 118, Steps taken: 1534, Time elapsed: 153.34066581726074, Total reward: -23.0\n",
      "Episode 119, Steps taken: 1259, Time elapsed: 125.89187574386597, Total reward: -2.0\n",
      "Episode 120, Steps taken: 1412, Time elapsed: 141.2162880897522, Total reward: -4.0\n",
      "Episode 121, Steps taken: 538, Time elapsed: 53.75160217285156, Total reward: -2.0\n",
      "Episode 122, Steps taken: 116, Time elapsed: 11.526138544082642, Total reward: -2.0\n",
      "Episode 123, Steps taken: 1354, Time elapsed: 135.330810546875, Total reward: -29.0\n",
      "Episode 124, Steps taken: 426, Time elapsed: 42.49095559120178, Total reward: -1.0\n",
      "Episode 125, Steps taken: 75, Time elapsed: 7.477701663970947, Total reward: -3.0\n",
      "Episode 126, Steps taken: 239, Time elapsed: 23.784871816635132, Total reward: -1.0\n",
      "Episode 127, Steps taken: 2269, Time elapsed: 226.8569712638855, Total reward: -22.0\n",
      "Episode 128, Steps taken: 145, Time elapsed: 14.454906702041626, Total reward: 1.0\n",
      "Episode 129, Steps taken: 448, Time elapsed: 44.73904728889465, Total reward: -6.0\n",
      "Episode 130, Steps taken: 908, Time elapsed: 90.7125768661499, Total reward: -10.0\n",
      "Episode 131, Steps taken: 388, Time elapsed: 38.72247862815857, Total reward: -5.0\n",
      "Episode 132, Steps taken: 1397, Time elapsed: 139.6473240852356, Total reward: 0.0\n",
      "Episode 133, Steps taken: 196, Time elapsed: 19.562954664230347, Total reward: 1.0\n",
      "Episode 134, Steps taken: 487, Time elapsed: 48.668970584869385, Total reward: -1.0\n",
      "Episode 135, Steps taken: 932, Time elapsed: 93.12596487998962, Total reward: -18.0\n",
      "Episode 136, Steps taken: 1041, Time elapsed: 103.98192501068115, Total reward: -2.0\n",
      "Episode 137, Steps taken: 1647, Time elapsed: 164.61275053024292, Total reward: -81.0\n",
      "Episode 138, Steps taken: 514, Time elapsed: 51.36864352226257, Total reward: -9.0\n",
      "Episode 139, Steps taken: 537, Time elapsed: 53.58397912979126, Total reward: 1.0\n",
      "Episode 140, Steps taken: 1012, Time elapsed: 101.11382389068604, Total reward: -13.0\n",
      "Episode 141, Steps taken: 2007, Time elapsed: 200.64126181602478, Total reward: -3.0\n",
      "Episode 142, Steps taken: 1094, Time elapsed: 109.34671235084534, Total reward: -22.0\n",
      "Episode 143, Steps taken: 5441, Time elapsed: 543.9881455898285, Total reward: -88.0\n",
      "Episode 144, Steps taken: 894, Time elapsed: 89.2904748916626, Total reward: -27.0\n",
      "Episode 145, Steps taken: 1743, Time elapsed: 174.2945692539215, Total reward: -21.0\n",
      "Episode 146, Steps taken: 1803, Time elapsed: 180.14774203300476, Total reward: -63.0\n",
      "Episode 147, Steps taken: 1151, Time elapsed: 115.07754969596863, Total reward: 1.0\n",
      "Episode 148, Steps taken: 363, Time elapsed: 36.2104389667511, Total reward: 0.0\n",
      "Episode 149, Steps taken: 1040, Time elapsed: 103.93260836601257, Total reward: -7.0\n",
      "Episode 150, Steps taken: 3243, Time elapsed: 324.2119607925415, Total reward: -50.0\n",
      "Episode 151, Steps taken: 2817, Time elapsed: 281.56992411613464, Total reward: -34.0\n",
      "Episode 152, Steps taken: 426, Time elapsed: 42.62933015823364, Total reward: 0.0\n",
      "Episode 153, Steps taken: 1262, Time elapsed: 126.22443842887878, Total reward: -43.0\n",
      "Episode 154, Steps taken: 797, Time elapsed: 79.63403487205505, Total reward: -41.0\n",
      "Episode 155, Steps taken: 615, Time elapsed: 61.398000955581665, Total reward: 0.0\n",
      "Episode 156, Steps taken: 598, Time elapsed: 59.73069429397583, Total reward: -6.0\n",
      "Episode 157, Steps taken: 1890, Time elapsed: 188.93103623390198, Total reward: -27.0\n",
      "Episode 158, Steps taken: 1404, Time elapsed: 140.34918642044067, Total reward: -11.0\n",
      "Episode 159, Steps taken: 948, Time elapsed: 94.7275938987732, Total reward: -2.0\n",
      "Episode 160, Steps taken: 1186, Time elapsed: 118.47333216667175, Total reward: -13.0\n",
      "Episode 161, Steps taken: 2047, Time elapsed: 204.6356554031372, Total reward: -25.0\n",
      "Episode 162, Steps taken: 686, Time elapsed: 68.56876373291016, Total reward: -6.0\n",
      "Episode 163, Steps taken: 1222, Time elapsed: 122.1813223361969, Total reward: -22.0\n",
      "Episode 164, Steps taken: 178, Time elapsed: 17.7166428565979, Total reward: 0.0\n",
      "Episode 165, Steps taken: 229, Time elapsed: 22.819785118103027, Total reward: -3.0\n",
      "Episode 166, Steps taken: 3981, Time elapsed: 397.9805066585541, Total reward: -22.0\n",
      "Episode 167, Steps taken: 824, Time elapsed: 82.41217708587646, Total reward: -1.0\n",
      "Episode 168, Steps taken: 1088, Time elapsed: 108.8085732460022, Total reward: -37.0\n",
      "Episode 169, Steps taken: 444, Time elapsed: 44.33149433135986, Total reward: -3.0\n",
      "Episode 170, Steps taken: 740, Time elapsed: 73.91821384429932, Total reward: -33.0\n",
      "Episode 171, Steps taken: 1624, Time elapsed: 162.40691447257996, Total reward: -42.0\n",
      "Episode 172, Steps taken: 657, Time elapsed: 65.66921877861023, Total reward: -7.0\n",
      "Episode 173, Steps taken: 481, Time elapsed: 47.983107566833496, Total reward: -9.0\n",
      "Episode 174, Steps taken: 3054, Time elapsed: 305.32873153686523, Total reward: -58.0\n",
      "Episode 175, Steps taken: 7512, Time elapsed: 751.1219737529755, Total reward: -198.0\n",
      "Episode 176, Steps taken: 267, Time elapsed: 26.617043256759644, Total reward: -25.0\n",
      "Episode 177, Steps taken: 1343, Time elapsed: 134.22492575645447, Total reward: -22.0\n",
      "Episode 178, Steps taken: 293, Time elapsed: 29.281301021575928, Total reward: 0.0\n",
      "Episode 179, Steps taken: 66, Time elapsed: 6.539165258407593, Total reward: -2.0\n",
      "Episode 180, Steps taken: 1130, Time elapsed: 112.96413660049438, Total reward: -21.0\n",
      "Episode 181, Steps taken: 448, Time elapsed: 44.72132396697998, Total reward: -2.0\n",
      "Episode 182, Steps taken: 170, Time elapsed: 16.889554023742676, Total reward: -4.0\n",
      "Episode 183, Steps taken: 595, Time elapsed: 59.446887254714966, Total reward: 1.0\n",
      "Episode 184, Steps taken: 1753, Time elapsed: 175.28808975219727, Total reward: -7.0\n",
      "Episode 185, Steps taken: 595, Time elapsed: 59.486639976501465, Total reward: -6.0\n",
      "Episode 186, Steps taken: 598, Time elapsed: 59.74753403663635, Total reward: 0.0\n",
      "Episode 187, Steps taken: 1, Time elapsed: 0.2021622657775879, Total reward: 1.0\n",
      "Episode 188, Steps taken: 736, Time elapsed: 73.5337302684784, Total reward: -2.0\n",
      "Episode 189, Steps taken: 606, Time elapsed: 60.597554445266724, Total reward: -15.0\n",
      "Episode 190, Steps taken: 762, Time elapsed: 76.15270161628723, Total reward: -5.0\n",
      "Episode 191, Steps taken: 252, Time elapsed: 25.186492204666138, Total reward: -2.0\n",
      "Episode 192, Steps taken: 1610, Time elapsed: 160.9144344329834, Total reward: -16.0\n",
      "Episode 193, Steps taken: 1438, Time elapsed: 143.69481301307678, Total reward: -4.0\n",
      "Episode 194, Steps taken: 1397, Time elapsed: 139.63620805740356, Total reward: -48.0\n",
      "Episode 195, Steps taken: 2092, Time elapsed: 209.0571825504303, Total reward: -24.0\n",
      "Episode 196, Steps taken: 728, Time elapsed: 72.75406312942505, Total reward: -17.0\n",
      "Episode 197, Steps taken: 123, Time elapsed: 12.198190450668335, Total reward: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 198, Steps taken: 1061, Time elapsed: 105.98809051513672, Total reward: -4.0\n",
      "Episode 199, Steps taken: 1529, Time elapsed: 152.8378837108612, Total reward: -56.0\n",
      "Episode 200/400Episode 200, Steps taken: 4548, Time elapsed: 454.7270121574402, Total reward: -88.0\n",
      "Episode 201, Steps taken: 2158, Time elapsed: 215.74714255332947, Total reward: -39.0\n",
      "Episode 202, Steps taken: 2190, Time elapsed: 218.9100878238678, Total reward: -14.0\n",
      "Episode 203, Steps taken: 85, Time elapsed: 8.456557035446167, Total reward: -4.0\n",
      "Episode 204, Steps taken: 2012, Time elapsed: 201.05030488967896, Total reward: -54.0\n",
      "Episode 205, Steps taken: 1329, Time elapsed: 132.83111596107483, Total reward: -11.0\n",
      "Episode 206, Steps taken: 2377, Time elapsed: 237.62399220466614, Total reward: -108.0\n",
      "Episode 207, Steps taken: 350, Time elapsed: 34.93128848075867, Total reward: -3.0\n",
      "Episode 208, Steps taken: 1428, Time elapsed: 142.7144331932068, Total reward: -60.0\n",
      "Episode 209, Steps taken: 951, Time elapsed: 94.99836611747742, Total reward: -20.0\n",
      "Episode 210, Steps taken: 1649, Time elapsed: 164.86532545089722, Total reward: -3.0\n",
      "Episode 211, Steps taken: 817, Time elapsed: 81.59171319007874, Total reward: -10.0\n",
      "Episode 212, Steps taken: 2546, Time elapsed: 254.53131747245789, Total reward: -38.0\n",
      "Episode 213, Steps taken: 1348, Time elapsed: 134.7978491783142, Total reward: -18.0\n",
      "Episode 214, Steps taken: 2684, Time elapsed: 268.36053347587585, Total reward: -56.0\n",
      "Episode 215, Steps taken: 452, Time elapsed: 45.128204345703125, Total reward: -10.0\n",
      "Episode 216, Steps taken: 423, Time elapsed: 42.26518964767456, Total reward: 0.0\n",
      "Episode 217, Steps taken: 1108, Time elapsed: 110.76143717765808, Total reward: -1.0\n",
      "Episode 218, Steps taken: 594, Time elapsed: 59.340447425842285, Total reward: -5.0\n",
      "Episode 219, Steps taken: 860, Time elapsed: 85.9094557762146, Total reward: -8.0\n",
      "Episode 220, Steps taken: 772, Time elapsed: 77.089914560318, Total reward: -26.0\n",
      "Episode 221, Steps taken: 522, Time elapsed: 52.102176904678345, Total reward: -2.0\n",
      "Episode 222, Steps taken: 1501, Time elapsed: 150.11485695838928, Total reward: -17.0\n",
      "Episode 223, Steps taken: 2039, Time elapsed: 203.92081117630005, Total reward: -23.0\n",
      "Episode 224, Steps taken: 1233, Time elapsed: 123.30896973609924, Total reward: -6.0\n",
      "Episode 225, Steps taken: 2484, Time elapsed: 248.33562874794006, Total reward: -15.0\n",
      "Episode 226, Steps taken: 1081, Time elapsed: 108.09911227226257, Total reward: -19.0\n",
      "Episode 227, Steps taken: 783, Time elapsed: 78.23862791061401, Total reward: -15.0\n",
      "Episode 228, Steps taken: 1576, Time elapsed: 157.61110949516296, Total reward: -17.0\n",
      "Episode 229, Steps taken: 80, Time elapsed: 8.02157473564148, Total reward: 1.0\n",
      "Episode 230, Steps taken: 1982, Time elapsed: 199.14277148246765, Total reward: -28.0\n",
      "Episode 231, Steps taken: 248, Time elapsed: 24.760965824127197, Total reward: -6.0\n",
      "Episode 232, Steps taken: 626, Time elapsed: 62.519615173339844, Total reward: -14.0\n",
      "Episode 233, Steps taken: 967, Time elapsed: 96.55824375152588, Total reward: -19.0\n",
      "Episode 234, Steps taken: 1837, Time elapsed: 183.62687253952026, Total reward: -38.0\n",
      "Episode 235, Steps taken: 391, Time elapsed: 39.034383058547974, Total reward: -8.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-327-85c9d3ba92a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;31m#step,time elapsed, mean reward, std reward, cumulative reward, total steps, total time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m \u001b[0mQ_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpected_sarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;31m#this returns the Q_values for all the state-action pairs in the environment running from start till end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-327-85c9d3ba92a3>\u001b[0m in \u001b[0;36mexpected_sarsa\u001b[1;34m(env, num_episodes, alpha, gamma)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;31m#take action and observe r,s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mepisode_steps\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mdecision_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# agent performs internal updates based on sampled experience\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"communicator.exchange\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has exited.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[1;34m(self, poll_callback)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[1;31m# Got an acknowledgment from the connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    857\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlagents\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import mlagents\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "def e_greedy(Q, state, epsilon, num_actions):\n",
    "    policy_s = np.ones(num_actions) * epsilon / num_actions\n",
    "    #calculates a policy to be used to calc the action to take later\n",
    "    #policy_s represents the policy probabilities for all actions in the current state.\n",
    "    best = np.argmax(Q[state])\n",
    "    #best is the index of the action with the highest Q-value for the given state.\n",
    "    policy_s[best] = 1-epsilon + (epsilon/num_actions)\n",
    "    #The line policy_s[best] = 1-epsilon + (epsilon/nA[0]) assigns a new value to the policy probability of the best action.\n",
    "    #1 - epsilon represents the exploitation component of the policy, where the best action is given a higher probability.\n",
    "    #(epsilon/nA[0]) represents the exploration component of the policy, where a small probability is distributed equally among all actions to encourage exploration.\n",
    "    #By updating policy_s[best] with the new probability, you ensure that the best action has a higher probability of being selected while still allowing for some exploration.\n",
    "    if np.sum(Q[state]) > 0:\n",
    "        #checks if there are any non-zero Q-values for the given state. If there are non-zero Q-values, it implies that the Q-values have been updated for this state during the learning process.\n",
    "        action = np.random.choice(np.arange(num_actions), p=policy_s)\n",
    "        #is used to select an action randomly according to the policy probabilities (policy_s). The np.arange(num_actions) generates an array of action indices, and p=policy_s specifies the probabilities associated with each action. By using np.random.choice, an action is randomly selected according to the given probabilities.\n",
    "    else:\n",
    "        action = spec.action_spec.random_action(len(decision_steps))\n",
    "        # called to select a random action from the action space. spec.action_spec.random_action generates a random action based on the action specification defined for the environment. len(decision_steps) is used to determine the number of agents in the environment.\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_Q_expsarsa(alpha, gamma, num_actions, eps, Q, state, action, reward, next_state=None):\n",
    "    \"\"\"Returns updated Q-value for the most recent experience.\"\"\"\n",
    "    current = Q[state][action.discrete]         # estimate in Q-table (for current state, action pair)\n",
    "    policy_s = np.ones(num_actions) * eps / num_actions  # current policy (for next state S')\n",
    "    policy_s[np.argmax(Q[next_state])] = 1 - eps + (eps / num_actions) # greedy action\n",
    "    Qsa_next = np.dot(Q[next_state], policy_s)         # get value of state at next time step\n",
    "    target = reward + (gamma * Qsa_next)               # construct target\n",
    "    new_value = current + (alpha * (target - current)) # get updated value\n",
    "    return new_value\n",
    "\n",
    "\n",
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    action_space = env.behavior_specs[behavior_name].action_spec\n",
    "    if spec.action_spec.is_continuous():\n",
    "        continuous_action_space = action_space[0]\n",
    "        num_actions = sum(continuous_action_space)\n",
    "    else: #is_discrete()\n",
    "        discrete_action_space = action_space[1]\n",
    "        # action size is 4 since move forward backward rotate right left are 4 different actions\n",
    "        # actions space is 2,2,2,2 since each of them can do that or do nothing\n",
    "        num_actions = sum(discrete_action_space)\n",
    "\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(num_actions))\n",
    "        # Q is initialised in the format of Q[state]=np.zeros(num_actions)\n",
    "        # so after updating q values, the q values will replace the zeros\n",
    "        # so when you call Q[state][action], action is 0-7, this represents a specific state-action pair\n",
    "        # Q[state][action] returns the q value for that specific state-action pair\n",
    "        # q value is calculated in def update_Q_expsarsa()\n",
    "    \n",
    "    total_steps=0\n",
    "    training_start_time=time.time()\n",
    "    print(\"########## START TRAINING ##########\")\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        episode_rewards = 0\n",
    "        episode_steps=0\n",
    "        episode_start_time=time.time()\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush() # flushes the output buffer, ensuring that the printed message is immediately displayed in the console without any delay.\n",
    "        epsilon = 1/i_episode \n",
    "            #decays the epsilon\n",
    "            #epsilon is the exploration-exploitation trade-off parameter. It determines the probability of exploration versus exploitation.\n",
    "            #increase epsilon increase randomness, so epsilon high at the start to do more exploration and then reduce later to do more exploitation of past knowledge\n",
    "        env.reset() # begin the episode\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            # decision steps returns the agents requesting an action when the code is called\n",
    "            # terminal steps returns the agents that have already reached a terminal step when the code is called, terminal step means end of episode, its like this because one scene can have more than 1 agent\n",
    "        tracked_agent = -1 \n",
    "            # this is just assigning an index to the agent you want to track right now\n",
    "            # 1 env can have multiple agents, so -1 means settling each agent from back to front\n",
    "        done = False # For the tracked_agent\n",
    "        state = tuple(map(tuple, camera_input) for camera_input in decision_steps.obs)\n",
    "            # print(decision_steps.obs[0].shape) gives (1, 128, 128, 3)\n",
    "            # this refers to the input observation of the agent, depends on how you setup the scene\n",
    "            # the values are the RGB pixel values of the 128x128 pixels\n",
    "            # since only camera sensor attached with 128x128 RGB input, it is (1, 128, 128, 3)\n",
    "            # 1 refers to the single observation from 1 camera, so if 2 cameras, 1 256x256, 1 128x128, you need to resize the images, or easiest, keep it standardised in the scene setup\n",
    "            \n",
    "            #The map() function executes a specified function for each item in an iterable. The item is sent to the function as a parameter.\n",
    "            #here map() maps a tuple function to each observation in decision_steps.obs \n",
    "            #To access elements from a map object, you can convert it to a list or tuple first, and then use indexing to retrieve specific elements. so list(sample) works but sample[0] wouldnt\n",
    "            \n",
    "            #if only 1 camera in the scene, for o in decision_steps.obs gives the entire decision_steps.obs\n",
    "            \n",
    "        while not done: #for the current tracked_agent\n",
    "            if tracked_agent == -1 and len(decision_steps) >= 1: # means there is still agents requesting a decision and have not reached terminal step\n",
    "              tracked_agent = decision_steps.agent_id[0]\n",
    "                #assign the tracked_agent to the specific agent_id in the env\n",
    "                \n",
    "            #choose next action under e-greedy Q\n",
    "            action = e_greedy(Q, state, epsilon, num_actions)\n",
    "            #take action and observe r,s\n",
    "            env.set_actions(behavior_name, action)\n",
    "            env.step()\n",
    "            episode_steps+=1\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name) # agent performs internal updates based on sampled experience\n",
    "                #checks the updated situation with the agents in decision_steps and terminal_steps\n",
    "            next_state = tuple(map(tuple, camera_input) for camera_input in decision_steps.obs)\n",
    "            if tracked_agent in decision_steps:\n",
    "                reward = decision_steps[tracked_agent].reward\n",
    "                episode_rewards += decision_steps[tracked_agent].reward\n",
    "                    #add the reward of the current tracked agent to the cumulative reward\n",
    "                #Q(s,a) <-- Q(s,a) + a(R1 + g*sum(expected prob a|next state *Q(next_state, action)) - Q(s,a))\n",
    "#                 prev_Q = Q[state][action] #dont need this because prevQ is already Q and you are updaitng the next Q with Q[state][action] = update_Q_expsarsa(alpha, gamma, num_actions, epsilon, Q, state, action, reward, next_state)\n",
    "                    #The code calculates the previous Q-value for the current state-action pair using prev_Q = Q[state][action]. This value will be used later in the Q-value update equation.\n",
    "                eps_probs = np.ones(num_actions) * (1-epsilon) + (epsilon/num_actions)\n",
    "                Q[state][action.discrete] = update_Q_expsarsa(alpha, gamma, num_actions, epsilon, Q, state, action, reward, next_state)\n",
    "                state = next_state\n",
    "            if tracked_agent in terminal_steps:\n",
    "                done = True\n",
    "                episode_rewards += terminal_steps[tracked_agent].reward\n",
    "                episode_time=time.time()-episode_start_time\n",
    "                print(f\"Episode {i_episode}, Steps taken: {episode_steps}, Time elapsed: {episode_time}, Total reward: {episode_rewards}\") #this only applies to doing in steps, Mean reward: {np.mean(episode_rewards)}, Std of Reward: {np.mean(episode_rewards)}\")\n",
    "                break\n",
    "        total_steps+=episode_steps\n",
    "        total_time=time.time()-training_start_time\n",
    "    print(\"########## END TRAINING ##########\")\n",
    "    print(f\"Total steps taken: {total_steps}, Total time taken: {total_time}\")\n",
    "    return Q\n",
    "\n",
    "#step,time elapsed, mean reward, std reward, cumulative reward, total steps, total time\n",
    "Q_table = expected_sarsa(env, 400, .7, .7)\n",
    "#this returns the Q_values for all the state-action pairs in the environment running from start till end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "3fb3b147",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
